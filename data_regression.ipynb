{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pjpan/anaconda3/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# %load ./NNregression_v1.py\n",
    "\"\"\"\n",
    "@author: Aaron Sim\n",
    "Kaggle competition: How Much Did It Rain II\n",
    "\n",
    "Training regression model v1\n",
    "\"\"\"\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import lasagne\n",
    "import lasagne.layers as LL\n",
    "from lasagne.objectives import aggregate\n",
    "from lasagne.random import set_rng #, get_rng\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from NN_architectures import build_1Dregression_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9307f26f461c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "(num_epochs*epoch_size)//train_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################### Main ################################\n",
    "def do_regression(num_epochs=2, # No. of epochs to train\n",
    "                  init_file=None,  # Saved parameters to initialise training\n",
    "                  epoch_size=680780,  # Whole dataset size\n",
    "                  valid_size=34848,\n",
    "                  train_batch_multiple=10637,  # No. of minibatches per batch\n",
    "                  valid_batch_multiple=1089,  # No. of minibatches per batch\n",
    "                  train_minibatch_size=64, \n",
    "                  valid_minibatch_size=32,\n",
    "                  eval_multiple=50,  # No. of minibatches to ave. in report\n",
    "                  save_model=True,\n",
    "                  input_width=19,\n",
    "                  rng_seed=100009,\n",
    "                  cross_val=0,  # Cross-validation subset label\n",
    "                  dataver=1,  # Label for different runs/architectures/etc\n",
    "                  rate_init=1.0,\n",
    "                  rate_decay=0.999983):\n",
    "\n",
    "    ###################################################\n",
    "    ################# 0. User inputs ##################\n",
    "    ###################################################\n",
    "    for i in range(1,len(sys.argv)):\n",
    "        if sys.argv[i].startswith('-'):\n",
    "            option = sys.argv[i][1:]\n",
    "            if option == 'i': init_file = sys.argv[i+1]\n",
    "            elif option[0:2] == 'v=' : dataver = int(option[2:])\n",
    "            elif option[0:3] == 'cv=' : cross_val = int(option[3:])\n",
    "            elif option[0:3] == 'rs=' : rng_seed = int(option[3:])\n",
    "            elif option[0:3] == 'ri=' : rate_init = np.float32(option[3:])\n",
    "            elif option[0:3] == 'rd=' : rate_decay = np.float32(option[3:])\n",
    "                                \n",
    "    print(\"Running with dataver %s\" % (dataver))\n",
    "    print(\"Running with cross_val %s\" % (cross_val))\n",
    "    \n",
    "    \n",
    "    ###################################################\n",
    "    ############# 1. Housekeeping values ##############\n",
    "    ###################################################\n",
    "    # Batch size is possibly not equal to epoch size due to memory limits\n",
    "    train_batch_size = train_batch_multiple*train_minibatch_size \n",
    "    assert epoch_size >= train_batch_size\n",
    "    \n",
    "    # Number of times we expect the training/validation generator to be called\n",
    "    max_train_gen_calls = (num_epochs*epoch_size)//train_batch_size \n",
    "\n",
    "    # Number of evaluations (total minibatches / eval_multiple)\n",
    "    num_eval = max_train_gen_calls*train_batch_multiple / eval_multiple\n",
    "    \n",
    "    \n",
    "    ###################################################\n",
    "    ###### 2. Define model and theano variables #######\n",
    "    ###################################################\n",
    "    if rng_seed is not None:\n",
    "        print(\"Setting RandomState with seed=%i\" % (rng_seed))\n",
    "        rng = np.random.RandomState(rng_seed)\n",
    "        set_rng(rng)\n",
    "    \n",
    "    print(\"Defining variables...\")\n",
    "    index = T.lscalar() # Minibatch index\n",
    "    x = T.tensor3('x') # Inputs \n",
    "    y = T.fvector('y') # Target\n",
    "    \n",
    "    print(\"Defining model...\")\n",
    "    network_0 = build_1Dregression_v1(\n",
    "                        input_var=x,\n",
    "                        input_width=input_width,\n",
    "                        nin_units=12,\n",
    "                        h_num_units=[64,128,256,128,64],\n",
    "                        h_grad_clip=1.0,\n",
    "                        output_width=1\n",
    "                        )\n",
    "                        \n",
    "    if init_file is not None:\n",
    "        print(\"Loading initial model parametrs...\")\n",
    "        init_model = np.load(init_file)\n",
    "        init_params = init_model[init_model.files[0]]           \n",
    "        LL.set_all_param_values([network_0], init_params)\n",
    "        \n",
    "    \n",
    "    ###################################################                                \n",
    "    ################ 3. Import data ###################\n",
    "    ###################################################\n",
    "    # Loading data generation model parameters\n",
    "    print(\"Defining shared variables...\")\n",
    "    train_set_y = theano.shared(np.zeros(1, dtype=theano.config.floatX),\n",
    "                                borrow=True) \n",
    "    train_set_x = theano.shared(np.zeros((1,1,1), dtype=theano.config.floatX),\n",
    "                                borrow=True)\n",
    "    \n",
    "    valid_set_y = theano.shared(np.zeros(1, dtype=theano.config.floatX),\n",
    "                                borrow=True)\n",
    "    valid_set_x = theano.shared(np.zeros((1,1,1), dtype=theano.config.floatX),\n",
    "                                borrow=True)\n",
    "    \n",
    "    # Validation data (pick a single augmented instance, rand0 here)\n",
    "    print(\"Creating validation data...\")    \n",
    "    chunk_valid_data = np.load(\n",
    "        \"./valid/data_valid_augmented_cv%s_t%s_rand0.npy\" \n",
    "        % (cross_val, input_width)\n",
    "        ).astype(theano.config.floatX)\n",
    "    chunk_valid_answers = np.load(\n",
    "        \"./valid/data_valid_expected_cv%s.npy\" \n",
    "        % (cross_val)\n",
    "        ).astype(theano.config.floatX)     \n",
    "    \n",
    "    print(\"chunk_valid_answers.shape\", chunk_valid_answers.shape)\n",
    "    print(\"Assigning validation data...\")\n",
    "    valid_set_y.set_value(chunk_valid_answers[:])\n",
    "    valid_set_x.set_value(chunk_valid_data.transpose(0,2,1))\n",
    "    \n",
    "    # Create output directory\n",
    "    if not os.path.exists(\"output_cv%s_v%s\" % (cross_val, dataver)):\n",
    "        os.makedirs(\"output_cv%s_v%s\" % (cross_val, dataver))\n",
    "    \n",
    "    \n",
    "    ###################################################                                \n",
    "    ########### 4. Create Loss expressions ############\n",
    "    ###################################################\n",
    "    print(\"Defining loss expressions...\")\n",
    "    prediction_0 = LL.get_output(network_0) \n",
    "    train_loss = aggregate(T.abs_(prediction_0 - y.dimshuffle(0,'x')))\n",
    "    \n",
    "    valid_prediction_0 = LL.get_output(network_0, deterministic=True)\n",
    "    valid_loss = aggregate(T.abs_(valid_prediction_0 - y.dimshuffle(0,'x')))\n",
    "    \n",
    "    \n",
    "    ###################################################                                \n",
    "    ############ 5. Define update method  #############\n",
    "    ###################################################\n",
    "    print(\"Defining update choices...\")\n",
    "    params = LL.get_all_params(network_0, trainable=True)\n",
    "    learn_rate = T.scalar('learn_rate', dtype=theano.config.floatX)\n",
    "    \n",
    "    updates = lasagne.updates.adadelta(train_loss, params,\n",
    "                                       learning_rate=learn_rate)\n",
    "    \n",
    "    \n",
    "    ###################################################                                \n",
    "    ######### 6. Define train/valid functions #########\n",
    "    ###################################################    \n",
    "    print(\"Defining theano functions...\")\n",
    "    train_model = theano.function(\n",
    "        [index, learn_rate],\n",
    "        train_loss,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[(index*train_minibatch_size):\n",
    "                            ((index+1)*train_minibatch_size)],\n",
    "            y: train_set_y[(index*train_minibatch_size):\n",
    "                            ((index+1)*train_minibatch_size)]  \n",
    "        }\n",
    "    )\n",
    "    \n",
    "    validate_model = theano.function(\n",
    "        [index],\n",
    "        valid_loss,\n",
    "        givens={\n",
    "            x: valid_set_x[index*valid_minibatch_size:\n",
    "                            (index+1)*valid_minibatch_size],\n",
    "            y: valid_set_y[index*valid_minibatch_size:\n",
    "                            (index+1)*valid_minibatch_size]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ###################################################                                \n",
    "    ################ 7. Begin training ################\n",
    "    ###################################################  \n",
    "    print(\"Begin training...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    cum_iterations = 0\n",
    "    this_train_loss = 0.0\n",
    "    this_valid_loss = 0.0\n",
    "    best_valid_loss = np.inf\n",
    "    best_iter = 0\n",
    "    \n",
    "    train_eval_scores = np.empty(num_eval)\n",
    "    valid_eval_scores = np.empty(num_eval)\n",
    "    eval_index = 0\n",
    "    aug_index = 0\n",
    "    \n",
    "    for batch in range(max_train_gen_calls):\n",
    "        start_time = time.time()        \n",
    "        chunk_train_data = np.load(\n",
    "            \"./train/data_train_augmented_cv%s_t%s_rand%s.npy\" %\n",
    "            (cross_val, input_width, aug_index)\n",
    "            ).astype(theano.config.floatX)\n",
    "        chunk_train_answers = np.load(\n",
    "            \"./train/data_train_expected_cv%s.npy\" % \n",
    "            (cross_val)\n",
    "            ).astype(theano.config.floatX)     \n",
    "            \n",
    "        train_set_y.set_value(chunk_train_answers[:])\n",
    "        train_set_x.set_value(chunk_train_data.transpose(0, 2, 1))\n",
    "        \n",
    "        # Iterate over minibatches in each batch\n",
    "        for mini_index in xrange(train_batch_multiple):\n",
    "            this_rate = np.float32(rate_init*(rate_decay**cum_iterations))\n",
    "            this_train_loss += train_model(mini_index, this_rate)\n",
    "            cum_iterations += 1\n",
    "            \n",
    "            # Report loss \n",
    "            if (cum_iterations % eval_multiple == 0):\n",
    "                this_train_loss = this_train_loss / eval_multiple\n",
    "                this_valid_loss = np.mean([validate_model(i) for\n",
    "                                    i in xrange(valid_batch_multiple)])\n",
    "                train_eval_scores[eval_index] = this_train_loss\n",
    "                valid_eval_scores[eval_index] = this_valid_loss\n",
    "                \n",
    "                # Save report every five evaluations\n",
    "                if ((eval_index+1) % 5 == 0):\n",
    "                    np.savetxt(\n",
    "                        \"output_cv%s_v%s/training_scores.txt\" %\n",
    "                        (cross_val, dataver),\n",
    "                         train_eval_scores, fmt=\"%.5f\"\n",
    "                         )\n",
    "                    np.savetxt(\n",
    "                        \"output_cv%s_v%s/validation_scores.txt\" %\n",
    "                        (cross_val, dataver),\n",
    "                         valid_eval_scores, fmt=\"%.5f\"\n",
    "                         )\n",
    "                    np.savetxt(\n",
    "                        \"output_cv%s_v%s/last_learn_rate.txt\" %\n",
    "                        (cross_val, dataver),\n",
    "                        [np.array(this_rate)], fmt=\"%.5f\"\n",
    "                        )\n",
    "                \n",
    "                # Save model if best validation score\n",
    "                if (this_valid_loss < best_valid_loss):  \n",
    "                    best_valid_loss = this_valid_loss\n",
    "                    best_iter = cum_iterations-1\n",
    "                    \n",
    "                    if save_model:\n",
    "                        np.savez(\"output_cv%s_v%s/model.npz\" % \n",
    "                                 (cross_val, dataver),\n",
    "                                 LL.get_all_param_values(network_0))\n",
    "                    \n",
    "                # Reset evaluation reports\n",
    "                eval_index += 1\n",
    "                this_train_loss = 0.0\n",
    "                this_valid_loss = 0.0\n",
    "                \n",
    "        aug_index += 1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(\"Computing time for batch %d: %f\" % (batch, end_time-start_time))\n",
    "        \n",
    "    print(\"Best validation loss %f after %d epochs\" %\n",
    "          (best_valid_loss, (best_iter*train_minibatch_size//epoch_size)))\n",
    "    \n",
    "    del train_set_x, train_set_y, valid_set_x, valid_set_y\n",
    "    gc.collect()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with dataver 1\n",
      "Running with cross_val 0\n",
      "Setting RandomState with seed=100009\n",
      "Defining variables...\n",
      "Defining model...\n",
      "Defining shared variables...\n",
      "Creating validation data...\n",
      "chunk_valid_answers.shape (34848,)\n",
      "Assigning validation data...\n",
      "Defining loss expressions...\n",
      "Defining update choices...\n",
      "Defining theano functions...\n",
      "Begin training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pjpan/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:178: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/pjpan/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:179: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-dd8c989756ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdo_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-388d175ab675>\u001b[0m in \u001b[0;36mdo_regression\u001b[1;34m(num_epochs, init_file, epoch_size, valid_size, train_batch_multiple, valid_batch_multiple, train_minibatch_size, valid_minibatch_size, eval_multiple, save_model, input_width, rng_seed, cross_val, dataver, rate_init, rate_decay)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[0maug_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_train_gen_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         chunk_train_data = np.load(\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    do_regression()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
